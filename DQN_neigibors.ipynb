{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.gnn import MyModel, RGCN\n",
    "import torch\n",
    "from data.graph_loader import GraphLoader, load_highschool\n",
    "from torch_geometric.utils import to_networkx, to_undirected, degree\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "############################ DATA LOADING #########################\n",
    "device = torch.device(\"cuda:2\")\n",
    "edge_index = load_highschool()\n",
    "num_nodes = 70\n",
    "edge_index = to_undirected(edge_index, num_nodes=num_nodes)\n",
    "node_idx = torch.arange(num_nodes).to(device)\n",
    "graph = Data(x=node_idx.cpu(), edge_index=edge_index)\n",
    "nx_graph = to_networkx(graph)\n",
    "node_degrees = degree(graph.edge_index[0], num_nodes=num_nodes)\n",
    "random_walk_pe = torch.load(\"datasets/random_walk_pe_highschool_128.pt\").to(device)\n",
    "\n",
    "############################ MODEL #########################\n",
    "hidden_dim = 128\n",
    "lr = 1e-4\n",
    "# q_value = MyModel(num_nodes=num_nodes,\n",
    "#     hidden_dim=hidden_dim).to(device)\n",
    "# target_q_value = MyModel(num_nodes=num_nodes,\n",
    "#     hidden_dim=hidden_dim).to(device)\n",
    "q_value = RGCN(\n",
    "    num_relations=2,\n",
    "    input_dim=num_nodes,\n",
    "    hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "target_q_value = RGCN(\n",
    "    num_relations=2,\n",
    "    input_dim=num_nodes,\n",
    "    hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "\n",
    "# initailize the embeddings and models\n",
    "q_value.embeddings.weight.data = random_walk_pe\n",
    "target_q_value.load_state_dict(q_value.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(q_value.parameters(), lr=lr)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Dict, List, Tuple\n",
    "from utils.segment_tree import MinSegmentTree, SumSegmentTree\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        n_step: int = 1, \n",
    "        gamma: float = 0.99\n",
    "    ):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.int_)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.full(size, False, dtype=bool)\n",
    "        \n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        \n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "        \n",
    "        # make a n-step transition\n",
    "        rew, next_obs, done = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            # for N-step Learning\n",
    "            indices=idxs,\n",
    "        )\n",
    "    \n",
    "    def sample_batch_from_idxs(\n",
    "        self, idxs: np.ndarray\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "    \n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: deque, gamma: float\n",
    "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
    "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "\n",
    "        return rew, next_obs, done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import networkx as nx\n",
    "\n",
    "class NodeImmunization(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        budget,\n",
    "        device,\n",
    "        graph,\n",
    "        oracle_calls = 100,\n",
    "        ):\n",
    "        self.budget = budget\n",
    "        self.oracle_calls = oracle_calls\n",
    "        self.device = device\n",
    "        self.nx_g = graph\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        reward, done = self._take_action(action)    \n",
    "        ob = self._build_ob()\n",
    "\n",
    "        return ob, reward, done\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        self.x[action] = 1\n",
    "        done = self._check_done()\n",
    "        # compute reward and solution\n",
    "        deleted_nodes = (self.x == 1).long()\n",
    "        reward = self._reward_compute(deleted_nodes)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def _reward_compute(self, node_index):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        node_index = node_index.squeeze().cpu().numpy()\n",
    "        node_index = node_index.nonzero()[0]\n",
    "        eigen_drop = compute_total_degree(self.nx_g, node_index)\n",
    "        \n",
    "        # eigen_drop = self.evaluation(node_index)\n",
    "        new_reward = eigen_drop - self.old_reward\n",
    "        if new_reward< 0:\n",
    "            new_reward = 0\n",
    "        # print(self.old_reward, eigen_drop, new_reward)\n",
    "        self.old_reward = eigen_drop\n",
    "\n",
    "        \n",
    "            \n",
    "        return new_reward\n",
    "\n",
    "    def _check_done(self):\n",
    "        num_deleted = (self.x == 1).float()\n",
    "        return num_deleted.sum() == self.budget\n",
    "            \n",
    "    def _build_ob(self):\n",
    "        return self.x.float()\n",
    "        \n",
    "    def register(self, g, num_samples = 1):\n",
    "        self.g = g\n",
    "        self.num_samples = num_samples\n",
    "        self.g.to(self.device)\n",
    "        self.old_reward = 0\n",
    "        \n",
    "        num_nodes = self.g.x.shape[0]\n",
    "        self.x = torch.zeros(\n",
    "            num_nodes, \n",
    "            num_samples, \n",
    "            dtype = torch.long, \n",
    "            device = self.device\n",
    "            )\n",
    "        \n",
    "        # self.evaluation = ShieldValue(self.nx_g, self.device)\n",
    "        ob = self._build_ob()      \n",
    "        return ob\n",
    "    \n",
    "def compute_total_degree(graph, nodes):\n",
    "    unique_neighbors = set()\n",
    "    for node in nodes:\n",
    "        unique_neighbors.update(graph.neighbors(node))\n",
    "    # Remove the nodes of interest from the unique neighbors set\n",
    "    unique_neighbors.difference_update(nodes)\n",
    "    return len(unique_neighbors)\n",
    "\n",
    "class ShieldValue(torch.nn.Module):\n",
    "    def __init__(self, graph, device) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.G = graph\n",
    "        self.adj = nx.adjacency_matrix(self.G, dtype=float).tolil()\n",
    "        eigenval, eigenvec = sparse.linalg.eigsh(self.adj, k=1, which='LA')\n",
    "        self.eigenval = eigenval.item()\n",
    "        self.ori_eigenval = eigenval.item()\n",
    "\n",
    "        self.A = torch.from_numpy(self.adj.todense()).to(device)\n",
    "        self.eigenvec = torch.from_numpy(eigenvec).to(device).squeeze()\n",
    "\n",
    "\n",
    "    def forward(self, node_index):\n",
    "        eigenval = self.eigenval\n",
    "        eigenvec = self.eigenvec\n",
    "        adj = self.A\n",
    "\n",
    "        mask = (node_index == 1).squeeze()\n",
    "        term1 = 2*eigenval*(mask*(eigenvec**2)).sum()\n",
    "        masked_eigenvec = (eigenvec * mask).reshape(-1,1)\n",
    "        term2 = (adj * (masked_eigenvec @ masked_eigenvec.T)).sum()\n",
    "        eigendrop = term1 - term2\n",
    "        return eigendrop.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as dc\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "verbose = True\n",
    "discount_rate = 0.9\n",
    "max_episodes = 1000\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 1 / 900\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "max_grad_norm = 1.0\n",
    "update_target_iters = 1000\n",
    "\n",
    "# Enviroment\n",
    "env = NodeImmunization(\n",
    "    budget=16,\n",
    "    device=device,\n",
    "    graph=nx_graph,\n",
    "    )\n",
    "\n",
    "# Replay Buffer\n",
    "buffer = ReplayBuffer(num_nodes, size=5000, batch_size=batch_size, gamma=discount_rate)\n",
    "\n",
    "update_iters = 0\n",
    "eigen_vals = []\n",
    "total_loss = []\n",
    "for episode in range(1, max_episodes+1):\n",
    "        ob = env.register(graph)\n",
    "        done = False\n",
    "        mean_loss = []\n",
    "        while not done:\n",
    "            # Run policy to collect data\n",
    "            q_value.eval()\n",
    "            with torch.no_grad():\n",
    "                mask = (env.x == 0).squeeze()\n",
    "                \n",
    "                node_value = q_value(node_idx, graph.edge_index, ob)\n",
    "                \n",
    "                # choose action\n",
    "                if epsilon > np.random.random():\n",
    "                    randomly_selected_node = random.choice(range(mask.sum().item()))\n",
    "                    index = mask.nonzero().squeeze()[randomly_selected_node]\n",
    "                    action = index\n",
    "                else:\n",
    "                    selected_node = torch.argmax(node_value[mask]).item()\n",
    "                    index = mask.nonzero().squeeze()[selected_node]\n",
    "                    action = index\n",
    "                    \n",
    "                # take action\n",
    "                next_ob, reward, done = env.step(action)\n",
    "                \n",
    "                # store the trajectory\n",
    "                buffer.store(\n",
    "                    ob.squeeze().cpu().numpy(),\n",
    "                    action.item(),\n",
    "                    reward,\n",
    "                    next_ob.squeeze().cpu().numpy(),\n",
    "                    done.item()\n",
    "                )\n",
    "                ob = next_ob\n",
    "            #################################################################\n",
    "\n",
    "\n",
    "\n",
    "            count = 0\n",
    "            avg_loss = 0\n",
    "            for iters in range(1):\n",
    "                # only start to train when we have enough samples\n",
    "                if buffer.size - 8 > buffer.batch_size:\n",
    "\n",
    "                    ########## data preparation ###############################################\n",
    "                    verbose = True\n",
    "                    batch_data = buffer.sample_batch()\n",
    "                    batch_dataset = []\n",
    "\n",
    "                    for i in range(batch_data[\"obs\"].shape[0]):\n",
    "                        G = dc(graph).cpu()\n",
    "                        G.x = node_idx.cpu()\n",
    "                        next_obs = torch.from_numpy(batch_data['next_obs'][i])\n",
    "                        obs = torch.from_numpy(batch_data['obs'][i])\n",
    "                        G.obs = obs\n",
    "                        G.next_obs = next_obs\n",
    "                        G.rews = torch.tensor([batch_data['rews'][i]])\n",
    "                        G.y = torch.zeros(num_nodes, 1).squeeze()\n",
    "                        G.y[int(batch_data['acts'][i])] = 1\n",
    "                        G.done = torch.tensor([batch_data['done'][i]], dtype=torch.bool)\n",
    "                        G.indices = batch_data[\"indices\"][i]\n",
    "                        # G.weights = batch_data[\"weights\"][i]\n",
    "                \n",
    "                        batch_dataset.append(G)\n",
    "                    #########################################################################\n",
    "                    \n",
    "\n",
    "\n",
    "                    ################## Q-learning #############################################\n",
    "                    loader1 = DataLoader(batch_dataset, batch_size=batch_size, shuffle=False)\n",
    "                    for batch in loader1:\n",
    "                        # Target\n",
    "                        batch = batch.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            \n",
    "                            target = target_q_value(batch.x, batch.edge_index, batch.next_obs).detach()\n",
    "                            predicted_value_next = q_value(batch.x, batch.edge_index, batch.next_obs).detach()\n",
    "                            \n",
    "                            actions = []\n",
    "                            for i in range(len(batch.rews)):\n",
    "                                mask = (batch.batch == i).squeeze()\n",
    "                                feasible_actions = (batch.obs[mask] == 0).squeeze()\n",
    "                                selected_node = torch.argmax(predicted_value_next[mask][feasible_actions]).item()\n",
    "                                index = feasible_actions.nonzero().squeeze()[selected_node].item()\n",
    "                                index = mask.nonzero().squeeze()[index].item()\n",
    "                                actions.append(index)   # choose a feasible action\n",
    "\n",
    "                            discounted_rewards = discount_rate*target[actions].squeeze()\n",
    "                            discounted_rewards[batch.done] = 0      # set to 0 if it it the last step\n",
    "                            target = batch.rews + discounted_rewards\n",
    "\n",
    "                        # Train\n",
    "                        q_value.train()\n",
    "                        predicted_value = q_value(batch.x, batch.edge_index, batch.obs) \n",
    "                        elementwise_loss = 0.5*((target - predicted_value[batch.y==1].squeeze())**2)\n",
    "                    \n",
    "                    loss = (elementwise_loss).mean()\n",
    "                    avg_loss += loss.detach().item()\n",
    "                    count += 1\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                            q_value.parameters(), \n",
    "                            max_grad_norm\n",
    "                            )\n",
    "                    optimizer.step()\n",
    "                    ######################################################################################\n",
    "                    started = True\n",
    "                else:\n",
    "                    started = False\n",
    "                    continue\n",
    "            # logging loss\n",
    "            mean_loss.append(avg_loss/(count+1e-8))\n",
    "\n",
    "            # Set two networks equal for every \"update_target_iters\" steps\n",
    "            update_iters += 1\n",
    "            if update_iters % update_target_iters == 0:\n",
    "                target_q_value.load_state_dict(q_value.state_dict())\n",
    "                update_iters = 0\n",
    "\n",
    "            # Polyak averaging\n",
    "            # target_q_value.load_state_dict(polyak_avg(target_q_value.state_dict(), q_value.state_dict()))\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon = max(\n",
    "            min_epsilon, epsilon - (\n",
    "                max_epsilon - min_epsilon\n",
    "            ) * epsilon_decay\n",
    "        )\n",
    "\n",
    "        if started:\n",
    "            # logging the true eigen drop\n",
    "            G = dc(nx_graph)\n",
    "            G.remove_nodes_from(torch.where(ob == 1)[0].cpu().numpy())\n",
    "            adj = nx.adjacency_matrix(G, dtype=float)\n",
    "            eigenval, _ = sparse.linalg.eigsh(adj, k=1, which='LA')\n",
    "            eigen_vals.append(eigenval)\n",
    "            total_loss.append(np.mean(mean_loss))\n",
    "            \n",
    "            if verbose:\n",
    "                # torch.save(q_value.state_dict(), model_path)\n",
    "                # writer.writerow({\n",
    "                #     'episode': episode, \n",
    "                #     'avg loss': np.mean(mean_loss),\n",
    "                #     'eigenval': eigenval.item()})\n",
    "                print(\"Episode: {}/{} , Avg Loss: {:.4f}, Eigenval: {:.4f}, Mean: {:.4f}, STD: {:.4f}\".format(\n",
    "                episode, max_episodes, \n",
    "                np.mean(mean_loss), \n",
    "                eigenval.item(), \n",
    "                predicted_value.detach().mean().item(),\n",
    "                predicted_value.detach().std().item()\n",
    "                )\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
